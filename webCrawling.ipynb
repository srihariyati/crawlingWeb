{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import Library\r\n",
    "import requests\r\n",
    "from bs4 import BeautifulSoup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#membuat fungsi untuk scraping dan crawling\r\n",
    "def crawling_viva(tgl,bln,thn, x):\r\n",
    "    url = 'https://www.viva.co.id/indeks/berita/all/'+str(thn)+'/'+str(bln)+'/'+str(tgl)\r\n",
    "\r\n",
    "    #requst ke url indeks\r\n",
    "    req = requests.get(url)\r\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\r\n",
    "        \r\n",
    "    # menemukan link dari berita yang berada di href indeks\r\n",
    "    for linkberita in soup.find_all('a',attrs={\"class\":\"title-content\"}):\r\n",
    "        link_news = linkberita.get('href')\r\n",
    "\r\n",
    "        #membaca link full berita dengan menambah url \"?page=all&utm_medium=all-page\"\r\n",
    "        link_news_full = link_news+'?page=all&utm_medium=all-page'\r\n",
    "        \r\n",
    "        #test number of file\r\n",
    "        #print('---'+str(x)+'---'+link_news_full)\r\n",
    "        '''\r\n",
    "        # request ke link\r\n",
    "        r = requests.get(link_news_full, allow_redirects=True)\r\n",
    "        #download halaman ke local dengan format html\r\n",
    "        open('./downloadhtml/'+str(x+1)+'-viva.html', 'wb').write(r.content)\r\n",
    "        print('Dowloading ----> '+str(x)+ ':' + link_news_full)\r\n",
    "        '''\r\n",
    "        #menulis link kedalam file txt    \r\n",
    "        with open('./downloadhtml/daftar-link.txt', 'a') as f:\r\n",
    "            print('writing-->'+str(x)+'='+link_news_full)\r\n",
    "            f.write(link_news_full)\r\n",
    "            f.write('\\n')\r\n",
    "        \r\n",
    "        x+=1\r\n",
    "\r\n",
    "def crawling_kompas(tgl,bln,thn,x):\r\n",
    "    url = 'https://indeks.kompas.com/?site=all&date='+str(thn)+'-'+str(bln)+'-'+str(tgl)\r\n",
    "\r\n",
    "    #requst ke url indeks\r\n",
    "    req = requests.get(url)\r\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\r\n",
    "    # menemukan link dari berita yang berada di href indeks\r\n",
    "\r\n",
    "    for linkberita in soup.find_all('a',attrs={\"class\":\"article__link\"}):\r\n",
    "        link_news_full = linkberita.get('href')\r\n",
    "        \r\n",
    "        #test number of file\r\n",
    "        #print('---'+str(x)+'---'+link_news_full)\r\n",
    "        \r\n",
    "        # request ke link\r\n",
    "        r = requests.get(link_news_full, allow_redirects=True)\r\n",
    "        #download halaman ke local dengan format html\r\n",
    "        open('./downloadhtml/'+str(x)+'-kompas.html', 'wb').write(r.content)\r\n",
    "        print('Dowloading ----> '+str(x)+ ':' + link_news_full)\r\n",
    "        \r\n",
    "        #menulis link kedalam file txt    \r\n",
    "        with open('./downloadhtml/daftar-link.txt', 'a') as f:\r\n",
    "            print('writing-->'+str(x)+'='+link_news_full)\r\n",
    "            f.write(link_news_full)\r\n",
    "            f.write('\\n')\r\n",
    "        \r\n",
    "        x+=1 \r\n",
    "\r\n",
    "def crawling_gridoto(tgl,bln,thn,x):\r\n",
    "    url = 'https://www.gridoto.com/index?day='+str(tgl)+'&month='+str(bln)+'&year='+str(thn)+'&section=all'\r\n",
    "    #requst ke url indeks\r\n",
    "    req = requests.get(url)\r\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\r\n",
    "    # menemukan link dari berita yang berada di href indeks\r\n",
    "\r\n",
    "    for linkberita in soup.find_all('a',attrs={\"class\":\"news-list__link\"}):\r\n",
    "        link_news_full = linkberita.get('href')\r\n",
    "        \r\n",
    "        #test number of file\r\n",
    "        #print('---'+str(x)+'---'+link_news_full)\r\n",
    "        \r\n",
    "        # request ke link\r\n",
    "        r = requests.get(link_news_full, allow_redirects=True)\r\n",
    "        #download halaman ke local dengan format html\r\n",
    "        open('./downloadhtml/'+str(x)+'-gridito.html', 'wb').write(r.content)\r\n",
    "        print('Dowloading ----> '+str(x)+ ':' + link_news_full)\r\n",
    "        \r\n",
    "        #menulis link kedalam file txt    \r\n",
    "        with open('./downloadhtml/daftar-link.txt', 'a') as f:\r\n",
    "            f.write(link_news_full)\r\n",
    "            f.write('\\n')\r\n",
    "        \r\n",
    "\r\n",
    "        x+=1  \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "Crawling viva.com\r\n",
    "crawling 5 bulan\r\n",
    "timeline : 1 maret - 30 juli 2021   |\r\n",
    "1 halaman           = 22 berita     |\r\n",
    "1 hari 1 halaman    = 22 berita     |\r\n",
    "150 hari 1 halaman  = 3300 berita   |\r\n",
    "\r\n",
    "'''\r\n",
    "\r\n",
    "# memberikan nilai pada variabel\r\n",
    "h=1  \r\n",
    "x=0  # memberikan nama pada file berupa angka mulai dari 0\r\n",
    "b=3  \r\n",
    "t=2021  \r\n",
    "\r\n",
    "#c rawling dari 1 maret - 30 juli 2021- total ada 3300\r\n",
    "while(b<8) : \r\n",
    "    # Memanggil function viva\r\n",
    "    crawling_viva(h,b,t,x)\r\n",
    "    x=x+22\r\n",
    "    h+=1 \r\n",
    "    \r\n",
    "    # setelah tanggal 30 , bulan berganti\r\n",
    "    if(h>30): \r\n",
    "        b+=1\r\n",
    "        h=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "Crawling Kompas.com\r\n",
    "\r\n",
    "crawling 2 bulan\r\n",
    "timeline :1 april-30 mei 2021       |\r\n",
    "1 halaman           = 40 berita     |\r\n",
    "1 hari 1 halaman    = 40 berita     |\r\n",
    "60 hari 1 halaman   = 2400 berita   |\r\n",
    "'''\r\n",
    "#memberikan nilai pada variabel\r\n",
    "h=1\r\n",
    "x=3301  #memberikan nama pada file berupa angka lanjutan crawling viva\r\n",
    "b=4  \r\n",
    "t=2021  \r\n",
    "\r\n",
    "#crawling dari 1 april-30 mei 2021\r\n",
    "while(b<6) : \r\n",
    "    #Memanggil function \r\n",
    "    crawling_kompas(h,b,t,x)\r\n",
    "    x=x+40\r\n",
    "    h+=1 \r\n",
    "    \r\n",
    "    #setelah tanggal 30 , bulan berganti\r\n",
    "    if(h>30): \r\n",
    "        b+=1\r\n",
    "        h=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "Crawling GridOto.com\r\n",
    "\r\n",
    "timeline : 1 - 30 mei 2021          |\r\n",
    "1 halaman           = 15 berita     |\r\n",
    "1 hari 1 halaman    = 15 berita     |\r\n",
    "30 hari 1 halaman   = 450 berita   |\r\n",
    "'''\r\n",
    "\r\n",
    "#memberikan nilai pada variabel\r\n",
    "h=1\r\n",
    "x=5701  #memberikan nama pada file berupa angka dari file sebelumnya\r\n",
    "b=5  \r\n",
    "t=2021  \r\n",
    "\r\n",
    "#crawling dari 1-15 april dan 1-15 mei\r\n",
    "while(b<6) : \r\n",
    "    #Memanggil function \r\n",
    "    crawling_gridoto(h,b,t,x)\r\n",
    "    x=x+40\r\n",
    "    h+=1 \r\n",
    "    \r\n",
    "    #setelah tanggal 30 , bulan berganti\r\n",
    "    if(h>30): \r\n",
    "       b+=1"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ed225720166559e7176d3793db16a2fd8d295f725007103b21ac3099d2a89ee8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}